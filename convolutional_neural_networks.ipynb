{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformation this is to avoid overfitting\n",
    "\n",
    "# check the api doc for more info https://keras.io/api/preprocessing/image/\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,  # feature scaling to transform pixle to 0-1 (bc pixles are between 1-255)\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "        'Section 40 - Convolutional Neural Networks (CNN)/dataset/training_set',\n",
    "        target_size=(64, 64),  # final size that goes to the training\n",
    "        batch_size=32, # how many images in each batch\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the test set. Unlike the training dataset we will keep them intact expcept for the rescale\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "testing_set = test_datagen.flow_from_directory(\n",
    "        'Section 40 - Convolutional Neural Networks (CNN)/dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize cnn\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# add a convolutional layers\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu',\n",
    "    input_shape=(64, 64, 3)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Pooling - recall the max pooling (get the max from each result of the feature mapping)\n",
    "# purpose is to emphasize features and blur the image fpr generalization\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2, # 2 by 2\n",
    "    strides=2, # sliding 2 by 2 every time\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a second convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu',\n",
    "))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2, # 2 by 2\n",
    "    strides=2, # sliding 2 by 2 every time\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatterning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Connect to ANN which we learned before \n",
    "\n",
    "# add a dense layer\n",
    "dense = tf.keras.layers.Dense(units=128, activation='relu')  \n",
    "cnn.add(dense) # add layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sigmoid activation function so you will also get probability of the prediction\n",
    "# units set to 1\n",
    "output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')  \n",
    "cnn.add(output_layer) # add another layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling/Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.6789 - accuracy: 0.5679 - val_loss: 0.6414 - val_accuracy: 0.6365\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.6348 - accuracy: 0.6455 - val_loss: 0.6806 - val_accuracy: 0.6095\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.5873 - accuracy: 0.6948 - val_loss: 0.5812 - val_accuracy: 0.6970\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.5503 - accuracy: 0.7155 - val_loss: 0.5376 - val_accuracy: 0.7405\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.5230 - accuracy: 0.7371 - val_loss: 0.5129 - val_accuracy: 0.7630\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.4992 - accuracy: 0.7505 - val_loss: 0.4871 - val_accuracy: 0.7750\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.4836 - accuracy: 0.7661 - val_loss: 0.4967 - val_accuracy: 0.7610\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.4666 - accuracy: 0.7770 - val_loss: 0.5018 - val_accuracy: 0.7685\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.4539 - accuracy: 0.7839 - val_loss: 0.4819 - val_accuracy: 0.7790\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.4389 - accuracy: 0.7909 - val_loss: 0.4868 - val_accuracy: 0.7720\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.4201 - accuracy: 0.8040 - val_loss: 0.4701 - val_accuracy: 0.7840\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.4145 - accuracy: 0.8059 - val_loss: 0.4535 - val_accuracy: 0.7955\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3915 - accuracy: 0.8200 - val_loss: 0.4499 - val_accuracy: 0.8060\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.3794 - accuracy: 0.8298 - val_loss: 0.4955 - val_accuracy: 0.7755\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.3619 - accuracy: 0.8401 - val_loss: 0.4770 - val_accuracy: 0.7970\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.3539 - accuracy: 0.8407 - val_loss: 0.4631 - val_accuracy: 0.7985\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.3324 - accuracy: 0.8516 - val_loss: 0.4637 - val_accuracy: 0.7985\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.3127 - accuracy: 0.8646 - val_loss: 0.4786 - val_accuracy: 0.8045\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.2964 - accuracy: 0.8689 - val_loss: 0.5489 - val_accuracy: 0.7705\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.2891 - accuracy: 0.8783 - val_loss: 0.4975 - val_accuracy: 0.7785\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.2739 - accuracy: 0.8838 - val_loss: 0.5036 - val_accuracy: 0.8045\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.2631 - accuracy: 0.8911 - val_loss: 0.5146 - val_accuracy: 0.7865\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.2556 - accuracy: 0.8920 - val_loss: 0.4891 - val_accuracy: 0.8035\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.2313 - accuracy: 0.9075 - val_loss: 0.5658 - val_accuracy: 0.7885\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.2225 - accuracy: 0.9114 - val_loss: 0.6870 - val_accuracy: 0.7480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14a02e3c8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(\n",
    "    x=training_set,\n",
    "    validation_data=testing_set,\n",
    "    epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# use the helper function to load the image to make it acceptable by the model\n",
    "test_image = image.load_img(\n",
    "    'Section 40 - Convolutional Neural Networks (CNN)/dataset/single_prediction/cat_or_dog_1.jpg',\n",
    "    target_size=(64, 64)\n",
    ")\n",
    "\n",
    "# transform image to array\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# because the model is taking a batch of images we need to add another dimention\n",
    "# np.expand_dims will add an extra nested array\n",
    "# axis refers to the level of the nested structure, so 0 means the outer array\n",
    "test_image = np.expand_dims(test_image, axis=0) \n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "    \n",
    "print(prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('3.7.2': pyenv)",
   "language": "python",
   "name": "python37264bit372pyenv598c0904c91f42e8a121f9ab66f95843"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
